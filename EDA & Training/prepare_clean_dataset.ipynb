{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caeb576c-042a-479a-9a19-a24a766cc446",
   "metadata": {},
   "source": [
    "## üìÇ Section 1: Data Cleaning & Dataset Preparation\n",
    "\n",
    "In this section, I prepare a **clean and structured dataset** from the original rice disease image dataset provided in the assignment. The original dataset includes labeled images along with metadata such as `image_id`, `label` (disease), `variety`, and `age`.\n",
    "\n",
    "A clean dataset is essential for reliable model training. Therefore, I conduct several preprocessing steps to ensure **data integrity**, **consistency**, and **fair distribution** across classes.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step-by-Step Breakdown\n",
    "\n",
    "### 1. Imports and Configuration\n",
    "\n",
    "Here, I import all necessary Python libraries for:\n",
    "- **File handling** (`os`, `shutil`)\n",
    "- **Metadata management** (`pandas`)\n",
    "- **Image inspection** (`PIL.Image`)\n",
    "- **Dataset splitting** (`train_test_split`)\n",
    "- **Duplicate detection** (`hashlib.md5`)\n",
    "\n",
    "I also define constants such as the directory structure and the accepted image formats. This ensures maintainability and reduces hardcoding.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Metadata Loading\n",
    "\n",
    "I load the `meta_train.csv` file, which includes labels and metadata for each image. I convert the `image_id` to string to ensure consistent filename matching later on, especially if there are IDs with numeric formatting (e.g., `00001` vs `1`).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Duplicate ID Detection\n",
    "\n",
    "Before proceeding, I check for duplicate `image_id` entries in the metadata. Duplicate IDs could lead to **data leakage**, label conflicts, or redundancy. The goal is to confirm that each image in the dataset is uniquely identifiable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Label Folder Verification\n",
    "\n",
    "I compare the list of labels (disease names) in the CSV with the actual subfolders in the `train_images/` directory. This step helps identify:\n",
    "- Any **missing folders**\n",
    "- **Spelling inconsistencies** or formatting errors\n",
    "\n",
    "Why I do this: \n",
    "> Many deep learning pipelines (e.g., Keras `flow_from_directory`) rely on folder names as class labels. Mismatches can silently cause errors or mislabel data during training.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Image Existence Check\n",
    "\n",
    "I iterate through each row in the metadata and verify whether the image file actually exists at the expected location (`train_images/<label>/<image_id>`). Missing files are logged, and only valid paths are retained.\n",
    "\n",
    "This step protects against broken links and helps prevent `FileNotFoundError` during training.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Corrupted Image Detection\n",
    "\n",
    "Using PIL's `Image.verify()` method, I identify corrupted or unreadable images. These are **excluded** from the dataset to prevent:\n",
    "- Training crashes\n",
    "- Unexpected behavior in augmentation or batching\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Remove Corrupted Files\n",
    "\n",
    "After detecting corrupted images, I update the list of valid images and labels to **exclude them entirely**. This guarantees that downstream processes only work with healthy images.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Duplicate Image Content Check\n",
    "\n",
    "I calculate the **MD5 hash** of each image‚Äôs pixel data to detect visual duplicates (e.g., same image saved under different names). This is useful to:\n",
    "- Prevent overfitting from repeated data\n",
    "- Avoid artificially inflating model performance\n",
    "\n",
    "Note: In this script, I only *flag* duplicates ‚Äî I could extend it to remove them later if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Stratified Train/Validation Split\n",
    "\n",
    "I use `train_test_split` with stratification to split the cleaned data into 80% training and 20% validation sets. **Stratification ensures the same class distribution** across both sets, which is crucial in imbalanced datasets like this one.\n",
    "\n",
    "Why not use random split?\n",
    "> Random split might result in some disease classes being overrepresented in one set and underrepresented in the other ‚Äî leading to poor generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. File Copying\n",
    "\n",
    "Finally, I copy images into a new directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78d6f3a-67c9-4974-a5f0-12debdcd0b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLEAN] Loading metadata...\n",
      "[CLEAN] Duplicate image_id entries in CSV: 0\n",
      "[CLEAN] ‚úÖ All CSV labels match image folders.\n",
      "[CLEAN] Checking file existence...\n",
      "[CLEAN] ‚úÖ Valid image files found: 10407\n",
      "[CLEAN] Checking for corrupted images...\n",
      "[CLEAN] ‚úÖ Healthy images: 10407\n",
      "[CLEAN] ‚ùå Corrupted images: 0\n",
      "[CLEAN] Checking for duplicate image content...\n",
      "[CLEAN] ‚úÖ Unique image content: 10333\n",
      "[CLEAN] ‚ö†Ô∏è Duplicate image content found: 74\n",
      "[CLEAN] Splitting cleaned data into train/val (80/20)...\n",
      "[CLEAN] ‚úÖ Finished: dataset/train/ and dataset/val/ created with clean data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from hashlib import md5\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Run from MLmodels/\n",
    "SOURCE_DIR = os.path.join(BASE_PATH, \"train_images\")\n",
    "DEST_DIR = os.path.join(BASE_PATH, \"dataset\")\n",
    "META_CSV = os.path.join(BASE_PATH, \"meta_train.csv\")\n",
    "IMG_EXT = (\".jpg\", \".jpeg\", \".png\")\n",
    "LOG_PREFIX = \"[CLEAN]\"\n",
    "\n",
    "# === Load metadata ===\n",
    "print(f\"{LOG_PREFIX} Loading metadata...\")\n",
    "df = pd.read_csv(META_CSV)\n",
    "df['image_id'] = df['image_id'].astype(str)\n",
    "\n",
    "# === Check for duplicate metadata entries ===\n",
    "dupe_count = df['image_id'].duplicated().sum()\n",
    "print(f\"{LOG_PREFIX} Duplicate image_id entries in CSV: {dupe_count}\")\n",
    "\n",
    "# === Verify that label folders exist ===\n",
    "label_folders = set(os.listdir(SOURCE_DIR))\n",
    "csv_labels = set(df['label'].unique())\n",
    "invalid_labels = csv_labels - label_folders\n",
    "if invalid_labels:\n",
    "    print(f\"{LOG_PREFIX} ‚ùå Labels in CSV not found in image folders: {invalid_labels}\")\n",
    "else:\n",
    "    print(f\"{LOG_PREFIX} ‚úÖ All CSV labels match image folders.\")\n",
    "\n",
    "# === Check that images listed in metadata exist ===\n",
    "print(f\"{LOG_PREFIX} Checking file existence...\")\n",
    "valid_images = []\n",
    "valid_labels = []\n",
    "missing = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    img_name = row['image_id']\n",
    "    label = row['label']\n",
    "    path = os.path.join(SOURCE_DIR, label, img_name)\n",
    "    if os.path.isfile(path):\n",
    "        valid_images.append(path)\n",
    "        valid_labels.append(label)\n",
    "    else:\n",
    "        missing += 1\n",
    "\n",
    "print(f\"{LOG_PREFIX} ‚úÖ Valid image files found: {len(valid_images)}\")\n",
    "if missing:\n",
    "    print(f\"{LOG_PREFIX} ‚ùå Missing images: {missing}\")\n",
    "\n",
    "# === Check for corrupted/unreadable images ===\n",
    "print(f\"{LOG_PREFIX} Checking for corrupted images...\")\n",
    "corrupted_paths = []\n",
    "\n",
    "for path in valid_images:\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            img.verify()\n",
    "    except Exception:\n",
    "        corrupted_paths.append(path)\n",
    "\n",
    "print(f\"{LOG_PREFIX} ‚úÖ Healthy images: {len(valid_images) - len(corrupted_paths)}\")\n",
    "print(f\"{LOG_PREFIX} ‚ùå Corrupted images: {len(corrupted_paths)}\")\n",
    "\n",
    "# === Remove corrupted images from list ===\n",
    "valid_image_set = set(valid_images) - set(corrupted_paths)\n",
    "valid_images = [p for p in valid_images if p in valid_image_set]\n",
    "valid_labels = [l for p, l in zip(valid_images, valid_labels) if p in valid_image_set]\n",
    "\n",
    "# === Check for duplicate image content ===\n",
    "print(f\"{LOG_PREFIX} Checking for duplicate image content...\")\n",
    "hashes = {}\n",
    "duplicate_content = []\n",
    "\n",
    "for path in valid_images:\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            img_hash = md5(img.tobytes()).hexdigest()\n",
    "            if img_hash in hashes:\n",
    "                duplicate_content.append((path, hashes[img_hash]))\n",
    "            else:\n",
    "                hashes[img_hash] = path\n",
    "    except:\n",
    "        continue  # Already handled\n",
    "\n",
    "print(f\"{LOG_PREFIX} ‚úÖ Unique image content: {len(valid_images) - len(duplicate_content)}\")\n",
    "print(f\"{LOG_PREFIX} ‚ö†Ô∏è Duplicate image content found: {len(duplicate_content)}\")\n",
    "\n",
    "# === Final train/val split ===\n",
    "print(f\"{LOG_PREFIX} Splitting cleaned data into train/val (80/20)...\")\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    valid_images, valid_labels, test_size=0.2, stratify=valid_labels, random_state=42\n",
    ")\n",
    "\n",
    "# === Copy files to cleaned dataset ===\n",
    "def copy_files(paths, labels, subset):\n",
    "    for path, label in zip(paths, labels):\n",
    "        dest = os.path.join(DEST_DIR, subset, label)\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "        shutil.copy(path, dest)\n",
    "\n",
    "# === Clear old dataset if exists ===\n",
    "if os.path.exists(DEST_DIR):\n",
    "    shutil.rmtree(DEST_DIR)\n",
    "\n",
    "copy_files(train_paths, train_labels, \"train\")\n",
    "copy_files(val_paths, val_labels, \"val\")\n",
    "\n",
    "print(f\"{LOG_PREFIX} ‚úÖ Finished: dataset/train/ and dataset/val/ created with clean data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_env)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
